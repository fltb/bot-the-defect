# main.py

import os
from dotenv import load_dotenv
import json

# LlamaIndex v0.10+ æ–°ç‰ˆ API
from llama_index.core import VectorStoreIndex, load_index_from_storage
from llama_index.core import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.deepseek import DeepSeek
from llama_index.core.storage.storage_context import StorageContext  # Add missing import
from llama_index.core.vector_stores import (
    VectorStoreQuery,
    MetadataFilters,
    MetadataFilter,        # Or potentially ExactMatchFilter
    FilterCondition,
    FilterOperator
)

# è‡ªå®šä¹‰åŠ è½½å‡½æ•°
from loader import load_chunk_documents, load_background_documents

load_dotenv()
deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
if not deepseek_api_key:
    raise EnvironmentError("è¯·åœ¨ .env ä¸­è®¾ç½® DEEPSEEK_API_KEYã€‚")

# â€”â€”â€”â€” æ¨¡å‹é…ç½® â€”â€”â€”â€”
embed_model = HuggingFaceEmbedding(model_name="all-MiniLM-L6-v2")
llm = DeepSeek(model="deepseek-chat", api_key=deepseek_api_key)

Settings.llm = llm
Settings.embed_model = embed_model

# â€”â€”â€”â€” å­˜å‚¨ä¸Šä¸‹æ–‡ â€”â€”â€”â€”
storage_context = StorageContext.from_defaults()

# â€”â€”â€”â€” æ„å»ºæˆ–åŠ è½½ç´¢å¼• â€”â€”â€”â€”
if not os.path.exists("./storage"):
    print("æ­£åœ¨åˆå§‹åŒ–ç´¢å¼•â€¦")
    docs = load_chunk_documents() + load_background_documents()
    index = VectorStoreIndex.from_documents(
        docs,
        storage_context=storage_context,
        show_progress=True
    )
    # ä¿®æ­£ï¼šé¦–æ¬¡æŒä¹…åŒ–æ—¶æŒ‡å®šç›®å½•
    index.storage_context.persist(persist_dir="./storage")
else:
    print("åŠ è½½å·²æœ‰ç´¢å¼•â€¦")
    # ä¿®æ­£ï¼šåŠ è½½æ—¶æ˜ç¡®æŒ‡å®šæŒä¹…åŒ–ç›®å½•
    storage_context = StorageContext.from_defaults(persist_dir="./storage")
    index = load_index_from_storage(
        storage_context=storage_context,
    )


# â€”â€”â€”â€” 6. èŠå¤©å¾ªç¯ â€”â€”â€”â€”
def build_prompt(user_role, bot_role, history, question, retrieved_ctx):
    hist = "\n".join(f"{h['role']}: {h['content']}" for h in history)
    return f"""\
# æ¸¸æˆè®¾å®š
ä½ æ˜¯ {bot_role}ï¼Œä¸ä½ å¯¹è¯çš„ç©å®¶æ‰®æ¼” {user_role}ï¼Œè¯·ä¸¥æ ¼éµå®ˆè§’è‰²æ€§æ ¼è®¾å®šã€‚

# å†å²å¯¹è¯
{hist}

# åœºæ™¯æ£€ç´¢ä¸Šä¸‹æ–‡
{retrieved_ctx}

# ç©å®¶è¾“å…¥
{user_role}: {question}

# {bot_role} çš„å›å¤ï¼ˆä¿æŒè§’è‰²é£æ ¼ï¼‰:
"""

def main():
    print("ğŸŒŸ è§’è‰²æ‰®æ¼” ChatBot å¯åŠ¨")
    user_role = input("è¯·è¾“å…¥ç©å®¶è§’è‰²åï¼š")
    bot_role = input("è¯·è¾“å…¥ Chatbot æ‰®æ¼”çš„è§’è‰²åï¼š")
    filters = MetadataFilters(
        filters=[  # ä½¿ç”¨ 'filters' åˆ—è¡¨å‚æ•°
            MetadataFilter(
                key="roles",
                value=bot_role,
                operator=FilterOperator.CONTAINS 
            )
        ],
        condition=FilterCondition.AND  
    )    
    # â€”â€”â€”â€” 5. æ„å»ºæŸ¥è¯¢å¼•æ“ â€”â€”â€”â€”
    # ç›´æ¥åœ¨ as_query_engine ä¸­ä¼ å…¥ llmï¼ˆé€šè¿‡ Settingsï¼‰ :contentReference[oaicite:2]{index=2}
    query_engine = index.as_query_engine(
        similarity_top_k=6,
        filters=filters,
        response_mode="compact",
    )

    history = []
    while True:
        q = input(f"{user_role}: ")
        if q.lower() in ("exit","quit"):
            break

        # RAG æ£€ç´¢ä¸Šä¸‹æ–‡
        retrieved_nodes = query_engine.retrieve(q)
        retrieved = "\n".join(node.get_content() for node in retrieved_nodes)
        
        # ç»„è£… Prompt
        prompt = build_prompt(user_role, bot_role, history, q, retrieved)
        print(f'promt ${prompt}')
        # ç›´æ¥è°ƒç”¨ Settings.llmï¼ˆDeepSeekï¼‰ç”Ÿæˆ
        resp = llm.complete( prompt )

        print(f"{resp}\n")

        # æ›´æ–°å¯¹è¯å†å²
        history.append({"role": user_role, "content": q})
        history.append({"role": bot_role, "content": resp})

if __name__ == "__main__":
    main()
